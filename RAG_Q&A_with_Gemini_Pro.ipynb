{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a5dOuRS9YCTM-EtRKgcY4y-W-jEAPrVL",
      "authorship_tag": "ABX9TyPeMqfO4P5iZzS0bQ9S8Cja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinbluechip/RAG-Q-A-with-Gemini-Pro-Google-/blob/main/RAG_Q%26A_with_Gemini_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_pDY7LgngRS",
        "outputId": "5ffd239a-d354-49bc-85f7-7e738c61eed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.8/379.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q --upgrade google-generativeai langchain-google-genai chromadb pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "l1PbK30_oMzy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "giznog7DpAQ3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "GGsDf4hwpEf6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name = \"gemini-pro\")\n",
        "model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6h_NahQqE3n",
        "outputId": "d0a0d9a7-e21f-4014-d78f-9bb106474f82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"What are the usecases of LLMs?\")"
      ],
      "metadata": {
        "id": "HFX0tXp8qRnX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "YUno0wrMqZGe",
        "outputId": "9576f762-c3e5-4f27-b455-3a3d3f916493"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Business and Finance:**\n> \n> * **Customer service:** Providing personalized and automated responses to customer inquiries.\n> * **Content generation:** Creating high-quality content for marketing, social media, and advertising.\n> * **Market research:** Analyzing customer feedback and industry trends to identify opportunities.\n> * **Financial forecasting:** Predicting revenue, expenses, and investment returns.\n> * **Data analysis:** Extracting insights from large datasets to inform decision-making.\n> \n> **Education and Research:**\n> \n> * **Personalized learning:** Tailoring educational content and experiences to individual students' needs.\n> * **Academic writing assistance:** Generating abstracts, summaries, and research proposals.\n> * **Answering questions:** Providing quick and accurate answers to research and academic inquiries.\n> * **Language learning:** Enhancing vocabulary, grammar, and pronunciation.\n> * **Scientific discovery:** Assisting researchers in exploring hypotheses and generating new knowledge.\n> \n> **Healthcare:**\n> \n> * **Patient diagnosis:** Analyzing medical data to identify diseases and provide treatment recommendations.\n> * **Drug discovery:** Accelerating the development of new drugs and treatments.\n> * **Personalized medicine:** Tailoring treatments to individual patients based on their genetic and health history.\n> * **Clinical research:** Automating data collection, analysis, and reporting.\n> * **Patient education:** Providing easy-to-understand information about medical conditions and treatments.\n> \n> **Entertainment and Media:**\n> \n> * **Creative writing:** Generating ideas, writing scripts, and creating fictional characters.\n> * **Music composition:** Generating melodies, lyrics, and arrangements.\n> * **Video editing:** Automatically summarizing and enhancing video content.\n> * **Gaming:** Creating realistic and immersive game worlds and characters.\n> * **Art generation:** Generating visual content, including images, paintings, and sculptures.\n> \n> **Other Industries:**\n> \n> * **Government:** Automating administrative tasks, analyzing public sentiment, and informing policy decisions.\n> * **Legal:** Performing legal research, summarizing legal documents, and automating certain tasks.\n> * **Retail:** Providing personalized product recommendations, automating customer support, and optimizing inventory management.\n> * **Travel:** Generating travel itineraries, suggesting attractions, and booking accommodations.\n> * **Transportation:** Optimizing vehicle routing, predicting traffic patterns, and assisting with navigation."
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with Documents using RAG (Retreival Augment Generation)"
      ],
      "metadata": {
        "id": "8-oIzlVfq8yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "\n",
        "!sudo apt-get -y -qq install poppler-utils libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "sVlXM9H2q91O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ae035f-2871-40df-c126-9235b01136f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev tesseract-ocr\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 6 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 8,560 kB of archives.\n",
            "After this operation, 31.6 MB of additional disk space will be used.\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 123598 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libarchive-dev_3.6.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../1-libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../2-libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../3-tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../4-tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../5-tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 29.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123778 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../01-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package antiword.\n",
            "Preparing to unpack .../02-antiword_0.37-16_amd64.deb ...\n",
            "Unpacking antiword (0.37-16) ...\n",
            "Selecting previously unselected package flac.\n",
            "Preparing to unpack .../03-flac_1.3.3-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking flac (1.3.3-2ubuntu0.2) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../04-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../05-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../06-libgs9-common_9.55.0~dfsg1-0ubuntu5.9_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../07-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../08-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../09-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../10-libgs9_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../11-ghostscript_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package lame.\n",
            "Preparing to unpack .../12-lame_3.100-3build2_amd64.deb ...\n",
            "Unpacking lame (3.100-3build2) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../13-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../14-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../15-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../16-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../17-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../18-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../19-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../20-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../21-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libxslt1-dev:amd64.\n",
            "Preparing to unpack .../22-libxslt1-dev_1.1.34-4ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libxslt1-dev:amd64 (1.1.34-4ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package poppler-utils.\n",
            "Preparing to unpack .../23-poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Selecting previously unselected package pstotext.\n",
            "Preparing to unpack .../24-pstotext_1.9-6build1_amd64.deb ...\n",
            "Unpacking pstotext (1.9-6build1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../25-sox_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package swig4.0.\n",
            "Preparing to unpack .../26-swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../27-swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package unrtf.\n",
            "Preparing to unpack .../28-unrtf_0.21.10-clean-1_amd64.deb ...\n",
            "Unpacking unrtf (0.21.10-clean-1) ...\n",
            "Setting up unrtf (0.21.10-clean-1) ...\n",
            "Setting up libxslt1-dev:amd64 (1.1.34-4ubuntu0.22.04.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up lame (3.100-3build2) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up flac (1.3.3-2ubuntu0.2) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up antiword (0.37-16) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up pstotext (1.9-6build1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.27)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.96)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
            "Downloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-text-splitters, langchain\n",
            "Successfully installed langchain-0.2.12 langchain-text-splitters-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "k8Y9zDs5rXF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10900cf2-9647-4f20-d08d-01e05ace5b0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.12)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.23 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.27)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.96)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.10 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# restart python kernal if issues with langchain import."
      ],
      "metadata": {
        "id": "Rgv_nno8rCyq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "JzLLLsZprOgD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,\n",
        "                             temperature=0.2,convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "kXn9jFMltHSE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract text from the PDF\n",
        "pdf_loader = PyPDFLoader(\"/content/drive/MyDrive/RAG_documents/attention.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[3].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziudCPHVtIvX",
        "outputId": "1ba44ca1-58ed-4799-f45f-d7106d9a1185"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 1: The Transformer - model architecture.\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "3.1 Encoder and Decoder Stacks\n",
            "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
            "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
            "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
            "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512 .\n",
            "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position ican depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcyFH92WtMQu",
        "outputId": "76f9e750-0678-4cac-850e-92d740462767"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Pipeline: Embedding + Gemini (LLM)\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
        "texts = text_splitter.split_text(context)\n",
        "\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "_p5eJcUdtTR8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Describe the Multi-head attention layer in detail?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "4f8wdr1ftf62",
        "outputId": "b7e1222c-b82e-40ee-ed0b-8d9ce17882ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\n\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\n\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "3dHlFvWntlKH",
        "outputId": "c37c6bee-3cb9-4460-8a3d-3b23c5d07091"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\n\nWhere the projections are parameter matrices WQ\ni∈Rdmodel×dk,WK\ni∈Rdmodel×dk,WV\ni∈Rdmodel×dv\nandWO∈Rhdv×dmodel.\n\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality."
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n"
      ],
      "metadata": {
        "id": "9gqC3GiitqTB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"Describe the Multi-head attention layer in detail?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-E27Gz-DuBhd",
        "outputId": "b905bec7-4054-40a8-d506-bba699258b28"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multi-Head Attention consists of several attention layers running in parallel. Each attention layer is responsible for attending to a different subspace of the input. The outputs of the attention layers are then concatenated and projected to produce the final output.\\nThe Multi-Head Attention layer is defined as follows:\\n```\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\n```\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nThanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "E0PA6JeIuCw6",
        "outputId": "6f7286a5-37bb-4aa3-9470-b82313b4aef2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Multi-Head Attention consists of several attention layers running in parallel. Each attention layer is responsible for attending to a different subspace of the input. The outputs of the attention layers are then concatenated and projected to produce the final output.\nThe Multi-Head Attention layer is defined as follows:\n```\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\n```\nWhere the projections are parameter matrices WQ\ni∈Rdmodel×dk,WK\ni∈Rdmodel×dk,WV\ni∈Rdmodel×dv\nandWO∈Rhdv×dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nThanks for asking!"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Describe Random forest?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "Markdown(result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "3GeSKi9-uG_M",
        "outputId": "7cb4a97e-9000-4c8c-88fc-5de076bca97d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Random forest is a supervised learning algorithm that builds an ensemble of decision trees and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees."
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zt6EmoaWuUZq"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}